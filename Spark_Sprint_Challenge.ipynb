{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Sprint Challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sulemc/spark-sc/blob/master/Spark_Sprint_Challenge.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "b-y9l5SxPLKQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Spark Sprint Challenge:"
      ]
    },
    {
      "metadata": {
        "id": "oXX0kIcBNJdq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question 1**:\n",
        "\n",
        "In this question, you will utilize the MapReduce paradigm  in SPARK to aggregate movie ratings. The dataset you will work with is the MovieLens dataset - 100k version of it. It can be accessed from here:\n",
        "\n",
        "http://files.grouplens.org/datasets/movielens/ml-100k.zip  -- download the dataset and unzip the archive (if you are running on Colab, you can use `!unzip`).\n",
        "\n",
        "The structure of the \"**u.data**\" file is described within the folloing README.txt\n",
        "\n",
        "http://files.grouplens.org/datasets/movielens/ml-100k-README.txt\n",
        "\n",
        "However for the sake of simplicity, I have cut and pasted the structire of the \"u.data\" file here:\n",
        "\n",
        "**u.data**     -- The full u data set, 100000 ratings by 943 users on 1682 items.\n",
        "                    Each user has rated at least 20 movies.  Users and items are\n",
        "                    numbered consecutively from 1.  The data is randomly\n",
        "                    ordered. This is a tab separated list of \n",
        "\t                  user id | movie id | rating | timestamp. \n",
        "                    The time stamps are unix seconds since 1/1/1970 UTC \n",
        "\n",
        "\n",
        "\n",
        "**Step 1**: Load the contents from the \"u.data\" into a RDD\n",
        "\n",
        "**Step 2: ** Leverage a map operation to rearrange the results from step 1. The order should be user id, rating and then item id. Wihtin the map operation, ensure that the elements are of type int, float and int respectively.\n",
        "\n",
        "For example after rearranging the first row of data, the result will be ==> 196, 3.0, 242\n",
        "\n",
        "**Step 3: ** Use the map and reduce operators to sum up all the ratings within the data set\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hk71iGms5mHU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question 2: **\n",
        "\n",
        "In this question, the ask is to uncover the most popular movie.  The objective is find the movie id which appears most frequently in the dataset. \n",
        "\n",
        "**Step 1: ** Read in the \"u.data\" file\n",
        "\n",
        "**Step 2:** Use the \"map\" operator to create a RDD which contains the respective movie id's and a value of 1\n",
        "\n",
        "**Step 3: ** Use the reduceByKey operator to sum the values by movie id\n",
        "\n",
        "**Step 4:** Sort the movie id's based on the number of occurences\n",
        "\n",
        "**Step 5: ** Output the results\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3kRDnvdH7_wb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question 3:**\n",
        "\n",
        "In this question, you will need to determine the avg. # of friends grouped by age\n",
        "\n",
        "**Dataset**: The dataset contains information on the **a)** Name, **b)** Age and **c)** # of Friends. The dataset is accessible via the following link: https://www.dropbox.com/s/xzw44ntsau1pr4m/friendsData.csv?raw=1\n",
        "\n",
        "**ASK**: Leverage the \"**groupByKey**\" operator to determine the avg. # of friends grouped by age\n",
        "\n",
        "**Step 1**: Read in the data into a RDD\n",
        "\n",
        "**Step 2**: Use the \"**map**\" operator to create a **key, value pair **where key is the age and the value is a tuple containing the # of friends and 1 - for example (33, (30,1)) \n",
        "\n",
        "**Step 3**: Utilize the \"**reduceByKey**\" operator to calculate the sum of friends by age\n",
        "\n",
        "**Step 4**: Compute the average of the # of friends by age\n",
        "\n",
        "**Step 5**: Output the results\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "z3FIuJf9f6Y_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question 4:**\n",
        "\n",
        "In this question, you will construct a Linear Regression model to take some predictions\n",
        "\n",
        "**Dataset**: It contains a set of x, y values where x represents the feature and the y represents the label. This is a made up dataset meant to provide practice on training and testing a ML model in Spark to make predictions\n",
        "\n",
        "https://www.dropbox.com/s/950mm5nqr533hsh/feature_label_dataset.txt?raw=1\n",
        "\n",
        "**Step 1: ** Read in the dataset into a RDD\n",
        "\n",
        "**Step 2: ** Split on the comma delimiter and convert (i.e map) the read in data to the following format ==> (x, Vectors.dense(y))\n",
        "\n",
        "**Step 3: ** Convert the RDD into a DataFrame format\n",
        "\n",
        "**Step 4:** Split the data into training and testing parts in the ratio 60% (train the model with 60% of the data) to 40% (test the model with 40% of the data)\n",
        "\n",
        "**Step 5: ** Create a Linear Regression Model and train it with the training dataset\n",
        "\n",
        "**Step 6:** Predict values using the test data\n",
        "\n",
        "**Step 7:** Output the predicted values and actual values (from the underlying data set) side-by-side to enable comparision\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "McUx5Vi52zHz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question 5 (Stretch Goal):** \n",
        "\n",
        "For this question, build a **Word2Vec** model on the body of text (i.e. https://www.dropbox.com/s/m6u5v1mt18crxtv/small_body_of_text_on_history.tsv?raw=1) that we explored in the project assignment.\n",
        "\n",
        "Note - this data may be too large to run in Colab - you can sample it, run locally, or find different data to apply these techniques to.\n",
        "\n",
        "**Step 1:** Read in the body of text\n",
        "\n",
        "**Step 2: ** Split the document based on the '\\t' delimiter; then, tokenize the \"body of text\" based on the 3rd element of th split document\n",
        "\n",
        "**Step 3:** Construct a Word2Vec model and fit it to the body of text \n",
        "\n",
        "**Step 4:** Output the dictionary or vocabulary constructed by the Word2Vec model\n",
        "\n",
        "**Step 5: ** Pick a couple of words of your choice and find 5 words that are synonyms of the words you choose\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}